{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "909d61e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import gymnasium as gym\n",
    "import gym\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import stable_baselines3\n",
    "from stable_baselines3 import PPO\n",
    "from typing import Callable, Dict, List, Optional, Tuple, Type, Union\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from torch import Tensor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ca39cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "action space: Discrete(2)\n",
      "initial observation: (array([ 0.01792948,  0.04256191,  0.01649971, -0.01536165], dtype=float32), {})\n",
      "action: 0\n",
      "next observation: [ 0.01878072 -0.15279274  0.01619248  0.2824811 ]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akash\\anaconda3\\envs\\Karli_Master\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "# create the cartpole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "print('observation space:', env.observation_space) # box observation\n",
    "print('action space:', env.action_space) # 0 - left, 1 - right\n",
    "\n",
    "# reset the environment to the initial state\n",
    "obs = env.reset()\n",
    "print('initial observation:', obs)\n",
    "\n",
    "action = env.action_space.sample()\n",
    "print('action:', action)\n",
    "\n",
    "# take an action on the environment\n",
    "obs, r, done, info, _ = env.step(action)\n",
    "print('next observation:', obs)\n",
    "print('reward:', r)\n",
    "print('done:', done)\n",
    "print('info:', info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c644cee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "ActorCriticPolicy(\n",
      "  (features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (pi_features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (vf_features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (mlp_extractor): MlpExtractor(\n",
      "    (policy_net): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "    (value_net): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (action_net): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akash\\anaconda3\\envs\\Karli_Master\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the agent using PPO with the tensorboard logging callback\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=\"./tensorboard/\")\n",
    "print(model.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6003e84b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "model.learn(total_timesteps=250000) #train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bf30a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "<stable_baselines3.ppo.ppo.PPO object at 0x000001CEE3AA0EB0>\n"
     ]
    }
   ],
   "source": [
    "# define the agent with custom network architecture\n",
    "model_kwargs = PPO('MlpPolicy', env, verbose=1,policy_kwargs = dict(activation_fn=th.nn.ReLU, net_arch=[32,16]),\n",
    "                   tensorboard_log=\"./tensorboard/\") #define your own network architecture\n",
    "print(model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33bdb903",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 500         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 678         |\n",
      "|    iterations           | 121         |\n",
      "|    time_elapsed         | 365         |\n",
      "|    total_timesteps      | 247808      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014697803 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.476      |\n",
      "|    explained_variance   | -5.71       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0268     |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.0078     |\n",
      "|    value_loss           | 9.72e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 500         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 676         |\n",
      "|    iterations           | 122         |\n",
      "|    time_elapsed         | 369         |\n",
      "|    total_timesteps      | 249856      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009941474 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.481      |\n",
      "|    explained_variance   | -4.55       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00593     |\n",
      "|    n_updates            | 1210        |\n",
      "|    policy_gradient_loss | -0.0027     |\n",
      "|    value_loss           | 0.000209    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 500          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 676          |\n",
      "|    iterations           | 123          |\n",
      "|    time_elapsed         | 372          |\n",
      "|    total_timesteps      | 251904       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038524475 |\n",
      "|    clip_fraction        | 0.119        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.492       |\n",
      "|    explained_variance   | -0.197       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00261     |\n",
      "|    n_updates            | 1220         |\n",
      "|    policy_gradient_loss | -0.00168     |\n",
      "|    value_loss           | 0.000243     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1960de28f48>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%capture\n",
    "model_kwargs.learn(total_timesteps=250000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dd1a24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a custom network class which uses Recurrent Networks.\n",
    "class CustomNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom network for policy and value function.\n",
    "    It receives as input the features extracted by the features extractor.\n",
    "\n",
    "    :param feature_dim: (int) dimension of the features extracted with the features_extractor\n",
    "    :param hidden_layer_dim: (int) number of neurons in the hidden layer\n",
    "    :param input_dim_rnn_pi: (int) input dim to the rnn of the policy network\n",
    "    :param input_dim_rnn_vf: (int) input dim to the rnn of the value network\n",
    "    :param last_layer_dim_pi: (int) number of units for the last layer of the policy network, input dim to the action net\n",
    "    :param last_layer_dim_vf: (int) number of units for the last layer of the value network, input dim to the value net\n",
    "    :param num_layers: (int) number of layers to use for rnn\n",
    "    :param rnn: (str) type of rnn, default LSTM\n",
    "    :param bidirectional: (bool) this is applicable only for rnn, default False\n",
    "    :param non_linearity: (str) activation function to be used with rnn\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            feature_dim: int,\n",
    "            actions: int=1,\n",
    "            hidden_layer_dim: int = 32,\n",
    "            input_dim_rnn_pi: int = 16,\n",
    "            input_dim_rnn_vf: int = 16,\n",
    "            last_layer_dim_pi: int = 8,  # also represents the hidden units in the rnn\n",
    "            last_layer_dim_vf: int = 8,  # also represents the hidden units in the rnn\n",
    "            num_layers: int = 1,\n",
    "            rnn: str = 'LSTM',\n",
    "            bidirectional: bool = False,\n",
    "            non_linearity: str = 'relu'\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # IMPORTANT:\n",
    "        # Save output dimensions, used to create the distributions\n",
    "        self.latent_dim_pi = last_layer_dim_pi\n",
    "        self.latent_dim_vf = last_layer_dim_vf\n",
    "\n",
    "        # rnn\n",
    "        self.rnn = rnn\n",
    "\n",
    "        # bidirectional\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        if not self.bidirectional:\n",
    "            hidden_size_pi = last_layer_dim_pi\n",
    "            hidden_size_vf = last_layer_dim_vf\n",
    "        else:\n",
    "            hidden_size_pi = int(last_layer_dim_pi / 2)\n",
    "            hidden_size_vf = int(last_layer_dim_vf / 2)\n",
    "\n",
    "        # Policy network\n",
    "\n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_layer_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layer_dim, input_dim_rnn_pi),\n",
    "            nn.ReLU(),\n",
    "            #nn.Linear(input_dim_rnn_pi, input_dim_rnn_pi),\n",
    "            #nn.ReLU()\n",
    "        )\n",
    "        if rnn == 'RNN':\n",
    "            self.policy_rnn = nn.RNN(\n",
    "                input_size=input_dim_rnn_pi, hidden_size=hidden_size_pi, num_layers=num_layers,\n",
    "                bidirectional=bidirectional, nonlinearity=non_linearity\n",
    "            )\n",
    "        elif rnn == 'LSTM':\n",
    "            self.policy_lstm = nn.LSTM(\n",
    "                input_size=input_dim_rnn_pi, hidden_size=hidden_size_pi, num_layers=num_layers,\n",
    "                bidirectional=bidirectional\n",
    "            )\n",
    "        elif rnn == 'GRU':\n",
    "            self.policy_gru = nn.GRU(\n",
    "                input_size=input_dim_rnn_pi, hidden_size=hidden_size_pi, num_layers=num_layers,\n",
    "                bidirectional=bidirectional\n",
    "            )\n",
    "\n",
    "        # Value network\n",
    "\n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_layer_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layer_dim, input_dim_rnn_vf),\n",
    "            nn.ReLU(),\n",
    "            #nn.Linear(input_dim_rnn_vf, input_dim_rnn_vf),\n",
    "            #nn.ReLU()\n",
    "        )\n",
    "        if rnn == 'RNN':\n",
    "            self.value_rnn = nn.RNN(\n",
    "                input_size=input_dim_rnn_vf, hidden_size=hidden_size_vf, num_layers=num_layers,\n",
    "                bidirectional=bidirectional, nonlinearity=non_linearity\n",
    "            )\n",
    "        elif rnn == 'LSTM':\n",
    "            self.value_lstm = nn.LSTM(\n",
    "                input_size=input_dim_rnn_vf, hidden_size=hidden_size_vf, num_layers=num_layers,\n",
    "                bidirectional=bidirectional\n",
    "            )\n",
    "        elif rnn == 'GRU':\n",
    "            self.value_gru = nn.GRU(\n",
    "                input_size=input_dim_rnn_vf, hidden_size=hidden_size_vf, num_layers=num_layers,\n",
    "                bidirectional=bidirectional\n",
    "            )\n",
    "\n",
    "    def forward(self, features: th.Tensor):\n",
    "        \"\"\"\n",
    "        :return: (th.Tensor, th.Tensor) latent_policy, latent_value of the specified network.\n",
    "            If all layers are shared, then ``latent_policy == latent_value``\n",
    "        \"\"\"\n",
    "        return self.forward_actor(features), self.forward_critic(features)\n",
    "\n",
    "    def forward_actor(self, features: th.Tensor):\n",
    "        \"\"\"\n",
    "        :return: (th.Tensor, th.Tensor) latent_policy, latent_value of the specified network.\n",
    "            If all layers are shared, then ``latent_policy == latent_value``\n",
    "        \"\"\"\n",
    "        policy_out = self.policy_net(features)\n",
    "        #policy_out = th.cat([policy_out, features], dim=-1)\n",
    "        #print(policy_out, 'policy out')\n",
    "        #print(len(policy_out))\n",
    "        if self.rnn == 'RNN':\n",
    "            policy_out, _ = self.policy_rnn(\n",
    "                policy_out.unsqueeze(1))  # input shape: (batch_size, seq_len, feature_dim)\n",
    "        elif self.rnn == 'LSTM':\n",
    "            policy_out, _ = self.policy_lstm(\n",
    "                policy_out.unsqueeze(1))  # input shape: (batch_size, seq_len, feature_dim)\n",
    "        elif self.rnn == 'GRU':\n",
    "            policy_out, _ = self.policy_gru(\n",
    "                policy_out.unsqueeze(1))  # input shape: (batch_size, seq_len, feature_dim)\n",
    "        policy_out = policy_out[:, -1, :]  # only keep the output of the last time step\n",
    "        return policy_out\n",
    "\n",
    "    def forward_critic(self, features: th.Tensor) -> th.Tensor:\n",
    "        value_out = self.value_net(features)\n",
    "        #value_out = th.cat([value_out, features], dim=-1)\n",
    "        if self.rnn == 'RNN':\n",
    "            value_out, _ = self.value_rnn(value_out.unsqueeze(1))  # input shape: (batch_size, seq_len, feature_dim)\n",
    "        elif self.rnn == 'LSTM':\n",
    "            value_out, _ = self.value_lstm(value_out.unsqueeze(1))  # input shape: (batch_size, seq_len, feature_dim)\n",
    "        elif self.rnn == 'GRU':\n",
    "            value_out, _ = self.value_gru(value_out.unsqueeze(1))  # input shape: (batch_size, seq_len, feature_dim)\n",
    "        value_out = value_out[:, -1, :]  # only keep the output of the last time step\n",
    "\n",
    "        return value_out\n",
    "\n",
    "\n",
    "class CustomActorCriticPolicy(ActorCriticPolicy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: spaces.Space,\n",
    "        action_space: spaces.Space,\n",
    "        lr_schedule: Callable[[float], float],\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        super().__init__(\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            lr_schedule,\n",
    "            # Pass remaining arguments to base class\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "        # Disable orthogonal initialization\n",
    "        self.ortho_init = False\n",
    "\n",
    "    def _build_mlp_extractor(self) -> None:\n",
    "        self.mlp_extractor = CustomNetwork(self.features_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f573dd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "CustomActorCriticPolicy(\n",
      "  (features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (pi_features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (vf_features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (mlp_extractor): CustomNetwork(\n",
      "    (policy_net): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (policy_gru): GRU(16, 8)\n",
      "    (value_net): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (value_gru): GRU(16, 8)\n",
      "  )\n",
      "  (action_net): Linear(in_features=8, out_features=2, bias=True)\n",
      "  (value_net): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Use GRU architecture\n",
    "model_gru = PPO(CustomActorCriticPolicy, env, verbose=1, tensorboard_log=\"./tensorboard/\")\n",
    "print(model_gru.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad0c685c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 334         |\n",
      "|    ep_rew_mean          | 334         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 101         |\n",
      "|    time_elapsed         | 1135        |\n",
      "|    total_timesteps      | 206848      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004221526 |\n",
      "|    clip_fraction        | 0.048       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.479      |\n",
      "|    explained_variance   | -2.38e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24          |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0043     |\n",
      "|    value_loss           | 57          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 322          |\n",
      "|    ep_rew_mean          | 322          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 182          |\n",
      "|    iterations           | 102          |\n",
      "|    time_elapsed         | 1146         |\n",
      "|    total_timesteps      | 208896       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031995932 |\n",
      "|    clip_fraction        | 0.0341       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.522       |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 152          |\n",
      "|    n_updates            | 1010         |\n",
      "|    policy_gradient_loss | -0.00213     |\n",
      "|    value_loss           | 224          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 327         |\n",
      "|    ep_rew_mean          | 327         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 103         |\n",
      "|    time_elapsed         | 1157        |\n",
      "|    total_timesteps      | 210944      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006446613 |\n",
      "|    clip_fraction        | 0.0656      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.505      |\n",
      "|    explained_variance   | -2.38e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 52.5        |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.00465    |\n",
      "|    value_loss           | 129         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 334          |\n",
      "|    ep_rew_mean          | 334          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 182          |\n",
      "|    iterations           | 104          |\n",
      "|    time_elapsed         | 1169         |\n",
      "|    total_timesteps      | 212992       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059215017 |\n",
      "|    clip_fraction        | 0.0429       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.507       |\n",
      "|    explained_variance   | -2.38e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.99         |\n",
      "|    n_updates            | 1030         |\n",
      "|    policy_gradient_loss | -0.00234     |\n",
      "|    value_loss           | 33.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 337         |\n",
      "|    ep_rew_mean          | 337         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 105         |\n",
      "|    time_elapsed         | 1180        |\n",
      "|    total_timesteps      | 215040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004244437 |\n",
      "|    clip_fraction        | 0.0302      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.494      |\n",
      "|    explained_variance   | 3.58e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.64        |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | -0.00331    |\n",
      "|    value_loss           | 32.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 341         |\n",
      "|    ep_rew_mean          | 341         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 106         |\n",
      "|    time_elapsed         | 1192        |\n",
      "|    total_timesteps      | 217088      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007175316 |\n",
      "|    clip_fraction        | 0.0807      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.481      |\n",
      "|    explained_variance   | -1.43e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.7         |\n",
      "|    n_updates            | 1050        |\n",
      "|    policy_gradient_loss | -0.00442    |\n",
      "|    value_loss           | 7.47        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 343          |\n",
      "|    ep_rew_mean          | 343          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 182          |\n",
      "|    iterations           | 107          |\n",
      "|    time_elapsed         | 1203         |\n",
      "|    total_timesteps      | 219136       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048199184 |\n",
      "|    clip_fraction        | 0.0278       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.519       |\n",
      "|    explained_variance   | -5.25e-06    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.32         |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | -0.00126     |\n",
      "|    value_loss           | 6.73         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 331          |\n",
      "|    ep_rew_mean          | 331          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 182          |\n",
      "|    iterations           | 108          |\n",
      "|    time_elapsed         | 1214         |\n",
      "|    total_timesteps      | 221184       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030983458 |\n",
      "|    clip_fraction        | 0.0425       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.549       |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 50.8         |\n",
      "|    n_updates            | 1070         |\n",
      "|    policy_gradient_loss | -0.00393     |\n",
      "|    value_loss           | 112          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 338          |\n",
      "|    ep_rew_mean          | 338          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 182          |\n",
      "|    iterations           | 109          |\n",
      "|    time_elapsed         | 1225         |\n",
      "|    total_timesteps      | 223232       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059822705 |\n",
      "|    clip_fraction        | 0.0444       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.531       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 146          |\n",
      "|    n_updates            | 1080         |\n",
      "|    policy_gradient_loss | -0.0056      |\n",
      "|    value_loss           | 292          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 339         |\n",
      "|    ep_rew_mean          | 339         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 110         |\n",
      "|    time_elapsed         | 1237        |\n",
      "|    total_timesteps      | 225280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005961042 |\n",
      "|    clip_fraction        | 0.0285      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.52       |\n",
      "|    explained_variance   | -2.38e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 48.9        |\n",
      "|    n_updates            | 1090        |\n",
      "|    policy_gradient_loss | -0.00111    |\n",
      "|    value_loss           | 59          |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 347          |\n",
      "|    ep_rew_mean          | 347          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 181          |\n",
      "|    iterations           | 111          |\n",
      "|    time_elapsed         | 1251         |\n",
      "|    total_timesteps      | 227328       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031463755 |\n",
      "|    clip_fraction        | 0.0222       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.535       |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.83         |\n",
      "|    n_updates            | 1100         |\n",
      "|    policy_gradient_loss | -0.00504     |\n",
      "|    value_loss           | 86           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 354          |\n",
      "|    ep_rew_mean          | 354          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 181          |\n",
      "|    iterations           | 112          |\n",
      "|    time_elapsed         | 1263         |\n",
      "|    total_timesteps      | 229376       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014679721 |\n",
      "|    clip_fraction        | 0.0241       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.526       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.2         |\n",
      "|    n_updates            | 1110         |\n",
      "|    policy_gradient_loss | -0.0032      |\n",
      "|    value_loss           | 32.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 368          |\n",
      "|    ep_rew_mean          | 368          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 181          |\n",
      "|    iterations           | 113          |\n",
      "|    time_elapsed         | 1275         |\n",
      "|    total_timesteps      | 231424       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037229722 |\n",
      "|    clip_fraction        | 0.0235       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.516       |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.84         |\n",
      "|    n_updates            | 1120         |\n",
      "|    policy_gradient_loss | -0.000644    |\n",
      "|    value_loss           | 32.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 370          |\n",
      "|    ep_rew_mean          | 370          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 181          |\n",
      "|    iterations           | 114          |\n",
      "|    time_elapsed         | 1286         |\n",
      "|    total_timesteps      | 233472       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037822966 |\n",
      "|    clip_fraction        | 0.0265       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.515       |\n",
      "|    explained_variance   | 2.38e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 44.4         |\n",
      "|    n_updates            | 1130         |\n",
      "|    policy_gradient_loss | -0.00132     |\n",
      "|    value_loss           | 61.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 376          |\n",
      "|    ep_rew_mean          | 376          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 181          |\n",
      "|    iterations           | 115          |\n",
      "|    time_elapsed         | 1298         |\n",
      "|    total_timesteps      | 235520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034349812 |\n",
      "|    clip_fraction        | 0.0682       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.519       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 50.8         |\n",
      "|    n_updates            | 1140         |\n",
      "|    policy_gradient_loss | -0.00666     |\n",
      "|    value_loss           | 146          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 376          |\n",
      "|    ep_rew_mean          | 376          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 181          |\n",
      "|    iterations           | 116          |\n",
      "|    time_elapsed         | 1309         |\n",
      "|    total_timesteps      | 237568       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049357647 |\n",
      "|    clip_fraction        | 0.0527       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.542       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 64.5         |\n",
      "|    n_updates            | 1150         |\n",
      "|    policy_gradient_loss | -0.00522     |\n",
      "|    value_loss           | 89.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 356         |\n",
      "|    ep_rew_mean          | 356         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 181         |\n",
      "|    iterations           | 117         |\n",
      "|    time_elapsed         | 1321        |\n",
      "|    total_timesteps      | 239616      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005677114 |\n",
      "|    clip_fraction        | 0.0447      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.513      |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.7        |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0021     |\n",
      "|    value_loss           | 32.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 359          |\n",
      "|    ep_rew_mean          | 359          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 181          |\n",
      "|    iterations           | 118          |\n",
      "|    time_elapsed         | 1332         |\n",
      "|    total_timesteps      | 241664       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068821795 |\n",
      "|    clip_fraction        | 0.0734       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.514       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 40.7         |\n",
      "|    n_updates            | 1170         |\n",
      "|    policy_gradient_loss | -0.00534     |\n",
      "|    value_loss           | 232          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 367          |\n",
      "|    ep_rew_mean          | 367          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 181          |\n",
      "|    iterations           | 119          |\n",
      "|    time_elapsed         | 1344         |\n",
      "|    total_timesteps      | 243712       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060377438 |\n",
      "|    clip_fraction        | 0.0601       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.501       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 44           |\n",
      "|    n_updates            | 1180         |\n",
      "|    policy_gradient_loss | -0.0052      |\n",
      "|    value_loss           | 146          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 376         |\n",
      "|    ep_rew_mean          | 376         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 181         |\n",
      "|    iterations           | 120         |\n",
      "|    time_elapsed         | 1355        |\n",
      "|    total_timesteps      | 245760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005444856 |\n",
      "|    clip_fraction        | 0.0572      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.485      |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.7        |\n",
      "|    n_updates            | 1190        |\n",
      "|    policy_gradient_loss | -0.00537    |\n",
      "|    value_loss           | 88          |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 380         |\n",
      "|    ep_rew_mean          | 380         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 181         |\n",
      "|    iterations           | 121         |\n",
      "|    time_elapsed         | 1367        |\n",
      "|    total_timesteps      | 247808      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002889786 |\n",
      "|    clip_fraction        | 0.0133      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.476      |\n",
      "|    explained_variance   | -2.26e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.24        |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.00343    |\n",
      "|    value_loss           | 4.26        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 380          |\n",
      "|    ep_rew_mean          | 380          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 181          |\n",
      "|    iterations           | 122          |\n",
      "|    time_elapsed         | 1378         |\n",
      "|    total_timesteps      | 249856       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018543717 |\n",
      "|    clip_fraction        | 0.00854      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.48        |\n",
      "|    explained_variance   | -2.86e-06    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.02         |\n",
      "|    n_updates            | 1210         |\n",
      "|    policy_gradient_loss | -0.00128     |\n",
      "|    value_loss           | 3.66         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 382          |\n",
      "|    ep_rew_mean          | 382          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 181          |\n",
      "|    iterations           | 123          |\n",
      "|    time_elapsed         | 1390         |\n",
      "|    total_timesteps      | 251904       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041160136 |\n",
      "|    clip_fraction        | 0.0277       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.467       |\n",
      "|    explained_variance   | -3.34e-06    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.88         |\n",
      "|    n_updates            | 1220         |\n",
      "|    policy_gradient_loss | -0.00402     |\n",
      "|    value_loss           | 3.14         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x19610084a88>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%capture\n",
    "model_gru.learn(total_timesteps=250000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e9fe268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "CustomActorCriticPolicy(\n",
      "  (features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (pi_features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (vf_features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (mlp_extractor): CustomNetwork(\n",
      "    (policy_net): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (policy_lstm): LSTM(16, 8)\n",
      "    (value_net): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (value_lstm): LSTM(16, 8)\n",
      "  )\n",
      "  (action_net): Linear(in_features=8, out_features=2, bias=True)\n",
      "  (value_net): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Use LSTM architecture\n",
    "model_lstm = PPO(CustomActorCriticPolicy, env, verbose=1, tensorboard_log=\"./tensorboard/\")\n",
    "print(model_lstm.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c67035d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 429          |\n",
      "|    ep_rew_mean          | 429          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 188          |\n",
      "|    iterations           | 121          |\n",
      "|    time_elapsed         | 1312         |\n",
      "|    total_timesteps      | 247808       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026971917 |\n",
      "|    clip_fraction        | 0.00674      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.553       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.59         |\n",
      "|    n_updates            | 1200         |\n",
      "|    policy_gradient_loss | -0.000768    |\n",
      "|    value_loss           | 3.6          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 428          |\n",
      "|    ep_rew_mean          | 428          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 188          |\n",
      "|    iterations           | 122          |\n",
      "|    time_elapsed         | 1323         |\n",
      "|    total_timesteps      | 249856       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029776478 |\n",
      "|    clip_fraction        | 0.0224       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.54        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.9         |\n",
      "|    n_updates            | 1210         |\n",
      "|    policy_gradient_loss | -0.00528     |\n",
      "|    value_loss           | 45           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 429         |\n",
      "|    ep_rew_mean          | 429         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 123         |\n",
      "|    time_elapsed         | 1334        |\n",
      "|    total_timesteps      | 251904      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003912165 |\n",
      "|    clip_fraction        | 0.023       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.548      |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.28        |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.00277    |\n",
      "|    value_loss           | 11          |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x19610071f88>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%capture\n",
    "model_lstm.learn(total_timesteps=250000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
